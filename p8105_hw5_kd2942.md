p8105 Hw#5
================
Kaylin De Silva
11-15-2024

``` r
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.4     ✔ readr     2.1.5
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.1
    ## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1
    ## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1
    ## ✔ purrr     1.0.2     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(dplyr)
library(rvest)
```

    ## 
    ## Attaching package: 'rvest'
    ## 
    ## The following object is masked from 'package:readr':
    ## 
    ##     guess_encoding

``` r
set.seed(1)
```

This chunk loads the tidyverse, rvest, and dplyr libraries and fixes the
output.

**Problem 1**

``` r
sim_birthday = function(n) {
  birthdays = sample(1:365, n, replace = TRUE)
  duplicated = length(birthdays) != length(unique(birthdays))
  
  return(duplicated)
}

result = sim_birthday(50)
print(result)
```

    ## [1] TRUE

Did not get to fully complete problem \#1.

**Problem 2**

``` r
#setting up the function
datasets = function(n = 30, mu = 0, sigma = 5) {
  
  sim_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma),
  )
  return(sim_data)
}

#defining output 
output = data.frame(mu=numeric(), mu_hat=numeric(),p_value=numeric())

#setting up a sequence to iterate over and a for loop body that applies the datasets function for each sequence element, performs a ttest, and saves the result
for (mu in 0:6) {
  for (i in 1:5000) {
    sim_data=
    datasets(n=30, mu = mu, sigma = 5)
    t_test_result = t.test(sim_data$x, mu=0)
    t_test = broom::tidy(t_test_result)
    output= bind_rows(output,
                      data.frame(
                        mu=mu,
                        mu_hat = t_test$estimate,
                        p_value = t_test$p.value))
    }
}

#viewing the head of the output to ensure that the values are reasonable
head(output)
```

    ##               mu     mu_hat    p_value
    ## mean of x...1  0  1.1375619 0.19272527
    ## mean of x...2  0  0.6572555 0.42271056
    ## mean of x...3  0  0.2537965 0.72045904
    ## mean of x...4  0 -2.3074597 0.01062893
    ## mean of x...5  0  1.6126341 0.10677462
    ## mean of x...6  0  0.3117091 0.75343136

This chunk fixes the population n and sd and generates 5000 datasets
that are normally distributed for each population mean (means ranging
from 0-6).The sample mean and p-value from a t-test were obtained and
saved for each dataset.

``` r
#creating a variable that indicates whether the null hypothesis of the t-test was rejected or not, and then creating a variable for power by getting the proportion of rejected tests after grouping them by true mean 

output = output|>
  mutate(
    conclusion = ifelse(
      p_value <0.05, "Reject", "Fail to Reject")) |>
      group_by(mu) |>
        mutate(
    power = 
      mean(conclusion == "Reject"))

#creating a plot to show the proportion of times the null was rejected on the y axis and the true mean on the x axis   
output|> 
  ggplot(aes(y=power, x=mu)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Power of the t-test for Different Values of μ",
    x = "True Value of μ",
    y = "Power (Proportion of Null Rejected)"
  )
```

![](p8105_hw5_kd2942_files/figure-gfm/power%20vs%20true%20mean%20plot-1.png)<!-- -->

This chunk plots the power of the ttest versus the true population mean.
The plot indicates that the test power increases as the true population
mean value increases. This makes sense given that a larger effect size
is associated with a higher statistical power. As the null hypothesis is
testing that the mean is equal to 0, the power of the test increases as
the true mean gets further from 0 (effect size increases).

``` r
#creating a variable for average estimate of mean
output_gg =output |>
  mutate(
    avg_mu_hat = mean(mu_hat)) 

#creating a data frame that only shows data for samples for which the null was rejected
output_gg_reject = output |> 
  filter(conclusion=="Reject") |>
   mutate(
     avg_mu_hat = mean(mu_hat))

#plotting the average estimate of the sample mean against the true value of the mean 
output_gg_plot = output_gg|>
  ggplot(aes(y=avg_mu_hat, x=mu)) + 
  geom_point() + 
  labs(
    title = "Average Sample Mean Against True Mean",
    x = "True Value of μ",
    y = "Average Estimate of Sample Mean")

#overlaying the above plot with a plot of the average estimate of the mean only in samples for which the null was rejected on the y axis and the true mean on the x axis
output_gg_total  =
  output_gg_plot +
  geom_point(data = output_gg_reject, aes(y=avg_mu_hat, x=mu)) +
  geom_line(data = output_gg, aes(y=avg_mu_hat, x=mu, color = "All samples")) +
  geom_line(data = output_gg_reject, aes(y=avg_mu_hat, x=mu, color = "Samples where null was rejected")) +
  scale_color_manual(values = c("All samples" = "blue", "Samples where null was rejected" = "red"), name = "Lines") +
  theme(legend.position = "bottom", 
        legend.title = element_text(face = "bold"), 
        legend.text = element_text(size = 12))

print(output_gg_total)
```

![](p8105_hw5_kd2942_files/figure-gfm/true%20value%20of%20mean%20against%20avg%20estimate%20of%20mean-1.png)<!-- -->
This chunk plots the average estimate of the mean (in all samples and
only in samples where the null was rejected) against the true mean
value.

The sample average of the mean across tests for which the null is
rejected is not approximately equal to the true value of the mean for
true mean values 0-3. This is because these are true mean values where
the null was less likely to be rejected (compared to the true mean of
6). As the true mean gets greater (i.e. effect size increases), the
sample average gets closer to the true value across tests for which the
null is rejected.

**Problem 3**

``` r
#loading csv
raw_washington_df = read.csv(file = "./homicide-data.csv")

#viewing variables
head(raw_washington_df)
```

    ##          uid reported_date victim_last victim_first victim_race victim_age
    ## 1 Alb-000001      20100504      GARCIA         JUAN    Hispanic         78
    ## 2 Alb-000002      20100216     MONTOYA      CAMERON    Hispanic         17
    ## 3 Alb-000003      20100601 SATTERFIELD      VIVIANA       White         15
    ## 4 Alb-000004      20100101    MENDIOLA       CARLOS    Hispanic         32
    ## 5 Alb-000005      20100102        MULA       VIVIAN       White         72
    ## 6 Alb-000006      20100126        BOOK    GERALDINE       White         91
    ##   victim_sex        city state      lat       lon           disposition
    ## 1       Male Albuquerque    NM 35.09579 -106.5386 Closed without arrest
    ## 2       Male Albuquerque    NM 35.05681 -106.7153      Closed by arrest
    ## 3     Female Albuquerque    NM 35.08609 -106.6956 Closed without arrest
    ## 4       Male Albuquerque    NM 35.07849 -106.5561      Closed by arrest
    ## 5     Female Albuquerque    NM 35.13036 -106.5810 Closed without arrest
    ## 6     Female Albuquerque    NM 35.15111 -106.5378        Open/No arrest

The data set has 52,179 observations and 12 columns.

``` r
#creating a city_state variable
washington_df = raw_washington_df |>
  mutate(
    city_state = paste(city, state, sep=", ")
  )

#summarizing total homicides and total unsolved homicides by grouping by city_state and creating a variable for unsolved homicides using the disposition variable
summary = washington_df |>
  group_by(city_state)|>
  mutate(
    unsolved = ifelse(disposition != "Closed by arrest", "TRUE", "FALSE")) |>
  summarize(
    total_homicide = n(),
    total_unsolved= sum(unsolved == "TRUE"))

#viewing results   
print(summary)  
```

    ## # A tibble: 51 × 3
    ##    city_state      total_homicide total_unsolved
    ##    <chr>                    <int>          <int>
    ##  1 Albuquerque, NM            378            146
    ##  2 Atlanta, GA                973            373
    ##  3 Baltimore, MD             2827           1825
    ##  4 Baton Rouge, LA            424            196
    ##  5 Birmingham, AL             800            347
    ##  6 Boston, MA                 614            310
    ##  7 Buffalo, NY                521            319
    ##  8 Charlotte, NC              687            206
    ##  9 Chicago, IL               5535           4073
    ## 10 Cincinnati, OH             694            309
    ## # ℹ 41 more rows

This chunk summarizes within cities to obtain the total number of
homicides and the number of unsolved homicides.

``` r
#isolating Baltimore to run prop.test function
baltimore_df = summary |>
  filter(city_state == "Baltimore, MD") 

#defining output
output_df = data.frame(proportion=numeric(), confidence_lower=numeric(), confidence_higher=numeric())

#running prop.test
baltimore_test=
  prop.test(baltimore_df$total_unsolved, baltimore_df$total_homicide)|>
  broom::tidy()

#saving output
output_df= bind_rows(output_df,data.frame(
                        proportion = baltimore_test$estimate,
                        confidence_lower = baltimore_test$conf.low,
                        confidence_higher = baltimore_test$conf.high))

#viewing results 
print(output_df)
```

    ##   proportion confidence_lower confidence_higher
    ## p  0.6455607        0.6275625         0.6631599

The proportion of unsolved homicides in Baltimore was 0.6456 (95% CI:
0.6276, 0.6632).

``` r
#setting up function
prop_test_full = function(total_unsolved, total_homicide) {
  test_result = prop.test(total_unsolved, total_homicide)
  test_result_tidy = broom::tidy(test_result)
  
  return(test_result_tidy)
}

#using the map2 function to apply the prop.test to each city_state
output_df_full = summary |>
  mutate(test_result=
           purrr::map2(total_unsolved, total_homicide, prop_test_full)) |>
  unnest(test_result) |>
  select(city_state, estimate, conf.low, conf.high) 
```

    ## Warning: There was 1 warning in `mutate()`.
    ## ℹ In argument: `test_result = purrr::map2(total_unsolved, total_homicide,
    ##   prop_test_full)`.
    ## Caused by warning in `prop.test()`:
    ## ! Chi-squared approximation may be incorrect

This chunk runs the prop.test for each of the cities in the data set and
extracts both the proportion of unsolved homicides and the confidence
interval for each. The warning was checked and seems to be the result of
small sample sizes in certain cities (i.e. Tulsa, AL).

``` r
#creating plot for estimates and CIs for each city using geom_errorbar for the CIs 
output_df_full|>
  ggplot(aes(x=reorder(city_state, estimate), y = estimate)) +
  geom_point(color = "blue")+
  geom_errorbar(aes(ymin=conf.low, ymax=conf.high))+
  coord_flip() +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Proportion of Unsolved Homicides",
    caption = "95% Confidence Interval indicated by error bars"
  ) +
  theme(
    axis.text.y = element_text(size = 7))
```

![](p8105_hw5_kd2942_files/figure-gfm/plot%20for%20estimates%20and%20CIs%20for%20each%20city-1.png)<!-- -->
This chunk creates a plot for estimates and CIs for each city and
organizes cities according to the proportion of unsolved homicides.
